# PSyclone and LFRic single node tutorial: OpenMP #

In this section of the tutorial you will see how to parallelise code
with OpenMP directives in PSyclone using the LFRic API.

## Example ##

We will be using the same example that was used in the distributed
memory part of the tutorial. This example is extracted from the LFRic
model and is one of the most computationally costly sections of the
LFRic dynamical core. Note that all of the executable code has been
removed apart from the invoke call that we are interested in.

Note, for the time being, please ignore the Fortran file provided in
the same directory as this `README.md` file. That will be used later.

It is probably worth reminding yourself of the content of the invoke call:

```fortran
    call invoke( setval_c(grad_p, 0.0_r_def),                                &
                 scaled_matrix_vector_kernel_type(grad_p, p, div_star,       &
                                                  hb_inv),                   &
                 enforce_bc_kernel_type( grad_p ),                           &
                 apply_variable_hx_kernel_type(                              &
                       Hp, grad_p, mt_lumped_inv, p,                         &
                       compound_div, p3theta, ptheta2, m3_exner_star,        &
                       tau_t, timestep_term) )
```

`setval_c` is a builtin that iterates over dofs,
`scaled_matrix_vector_kernel_type` and
`enforce_variable_hx_kernel_type` are user-supplied kernels that can
modify fields on any function space (both continuous and
discontinuous) and `apply_variable_hx_kernel_type` is a user-supplied
kernel that modifies a field on a discontinuous function space. (This
information is specified in the metadata for each kernel.) Kernels
that modify fields on any function space must be assumed to be
modifying fields on continuous spaces in order to ensure correctness.

## Add OpenMP parallel loop directives

Let's run PSyclone with our OpenMP script. Note, the path below is
relative to the directory in which this `README.md` file resides as are
all subsequent paths in this document. Note, at the moment we want to
use the helmholtz file provided in the `../code` directory, not the
one in the same directory as this `README.md` file.

```bash
    $ psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py ../code/helmholtz_solver_alg_mod.x90
```

From the psy-layer PSyIR that is output to the screen you should see
that only two of the four loops have been parallelised (i.e. have had
`Directive` nodes added).

Above the PSyIR output you should see some lines explaining why two of
the loops were not parallelised. This information is output from the
transformation that is used in the `omp_script.py` script:

```bash
Transformation Error: Error in DynamoOMPParallelLoopTrans transformation. The kernel has an argument with INC access. Colouring is required.
```

If you take a look at the PSyclone script `omp_script.py` you will see that we
try to apply OpenMP parallelisation to all loops, but if the
transformation does not let us do this and raises an
exception we print out the exception text.

```bash
    $ my_fav_editor omp_script.py
```

While you are looking at this script you will also see that rather
than using the `walk` method to iterate over all nodes of type `Loop`
as we have done in earlier tutorial sections (`schedule.walk(Loop)`)
we use a shortcut method built in to the node to do this instead
(`schedule.loops()`).

# Mixed mode MPI and OpenMP code #

Before moving on it is worth mentioning that you have just generated
mixed mode (MPI and OpenMP) parallel code here which many manually
parallelised codes do not support. If you just want to generate OpenMP
code you can switch off the distributed memory generation option as
you did in the previous tutorial section (i.e. add the `-nodm` option
to the `psyclone` command). If you like you could reduce the number of
halo exchanges generated by setting the `COMPUTE_ANNEXED_DOFS` switch
to `true` in the configuration file, again as you did in the previous
tutorial section. (Remember that to use a local config file you need
to add `--config psyclone.cfg` to the `psyclone` command). The point
here is that these options can be used interchangeably with OpenMP
parallelisation.

## Colouring ##

The error messages produced by the OpenMP transformation hint at why
it did not parallelise two of the loops. The reason is that these
loops iterate over cells and may update shared dofs. This means that
different iterations of the loop can contribute to the same dof value
in a field which means the iterations of the loop are not independent
of each other.

There are various solutions to this problem. The one typically taken
in finite element codes is to colour the loop. Colouring should
hopefully have already been explained to you so we will not go into
much detail here. Essentially loop iterations (cell columns) are
grouped together into sets that are independent of each other. Each of
these groups is called a colour. Iterations with the same colour can
be run in parallel but iterations with different colours can
not. Therefore colouring typically splits a loop into two loops, the
outmost one is sequential and iterates over each of the colours, the
inner one is parallel and iterates over all cell columns of the same
colour.

Let's colour the required loops. We want to colour a loop if it
modifies fields that are, or might be, continuous. The simplest way to
do this is to perform the inverse check i.e. colour if any modified
fields in the loop are not discontinuous (if it is not definitely
discontinuous then it is or might be continuous).

Add the following code next to the OpenMP parallel loop transformation
creation (`otrans`)

```python
    ctrans = Dynamo0p3ColourTrans()
```

and add the following code within the invoke for loop after the
`schedule` variable is created: (Note this does not replace the
existing "OpenMP" loop in the script - that should remain.)

```python
        const = LFRicConstants()
        for loop in schedule.loops():
            if loop.field_space.orig_name not in const.VALID_DISCONTINUOUS_NAMES:
                ctrans.apply(loop)
```

The `loop.field_space` method returns the information about which
function space is being used to determine the iteration space of the
loop. The `orig_name` method returns the original name of that
function space as specified in the kernel metadata. This is not a
particularly nice interface and will be improved in the future.

What happens when we run this?

The reason we get an exception in the colouring transformation is
because the first loop in the schedule (the one that contains the
builtin) may modify a field on a continuous function space, but it
iterates over dofs and therefore does not need to be coloured (as all
accesses to dofs are independent). The colouring transformation checks
for this case and therefore raises an exception.

Let's fix this problem by adding another clause to the if test to make
sure we only colour loops which iterate over cells. Replace the
existing loop containing `ctrans.apply` with this one:

```python
        const = LFRicConstants()
        for loop in schedule.walk(Loop):
            if loop.field_space.orig_name not in const.VALID_DISCONTINUOUS_NAMES \
               and loop.iteration_space == "cell_column":
                ctrans.apply(loop)
```

This is looking better. You should see the previously unparallelised
loops are now split into two and the inner one (iterations over a
single colour) is parallelised using OpenMP.

Notice that you have not had to change the OpenMP transformation
logic, it has just done the right thing. This is because the script
tries to parallelise all loops and the OpenMP transformation refuses
to parallelise a loop that it knows to be serial. It knows this as
PSyclone has specified that this loop is of type `colours`. You can
see various loop types in the psy-layer PSyIR that is output to
the screen.

To confirm that this is what is happening, take a look above the
psy-layer PSyIR code that is output to the screen and you will see
messages from the transformation saying that it will not parallelise a
loop that iterates over colours:

```bash
Transformation Error: Error in DynamoOMPParallelLoopTrans transformation. The requested loop is over colours and must be computed serially.
```

## Generated code ##

So far we have only looked at the PSyIR. Let's now take a look at the generated code

```bash
    $ my_fav_editor psy.f90
```

Notice that the OpenMP parallel directives have clauses specifying
what is `shared` and what is `private`. Normally the HPC expert would
have to work these out and they would need to be checked every time
the code is modified. PSyclone works them out and adds them
automatically. For example:

```fortran
!$omp parallel do default(shared), private(df), schedule(static)
```

The OpenMP parallel directives also default to a `static` schedule. The
OpenMP schedule can be changed when creating the transformation
object. Valid values are:

```bash
['runtime', 'static', 'dynamic', 'guided', 'auto']
```

Try setting the value and see if the generated code changes, e.g.:

```python
    otrans = DynamoOMPParallelLoopTrans(omp_schedule="dynamic")
```

Feel free to try to provide an invalid value, you should get an
exception.

Take a look at the generated coloured loops. These loops call LFRic
infrastructure functions to provide the bounds and the dofmaps in the
loop body now have "cmap" lookups in them. These changes are all taken
care of by PSyclone. However, note that PSyclone is not responsible
for working out the number of colours, the colour map etc - PSyclone
relies on the LFRic infrastructure to provide this information and
simply asks for it via the LFRic infrastructure's API.

## Reductions ##

The example we have been using does not make use of
reductions. Therefore let's modify the example slightly to add one
in. This has already been done in the `helmholtz_solver_alg_mod.x90`
algorithm file provided in the same directory as this `README.md`
file. The only difference to the original example (in `../code`) is
that an inner-product builtin has been added at the end of the invoke
call.

From this directory run:

```bash
    $ psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py helmholtz_solver_alg_mod.x90 -d ../code
```

The `-d` option tells PSyclone where to look for the kernel code(s). You
can try without it if you like. PSyclone will complain that it can't
find one of the required files.

The psy-layer PSyIR that is output to the screen should show that the
inner-product builtin has been parallelised. Take a look at the
generated code:

```bash
    $ my_fav_editor psy.f90
```

Notice that the OpenMP directive around the inner-product loop has a
reduction clause. PSyclone has determined that a reduction is required
(from the kernel metadata) and automatically added the appropriate
OpenMP clause. Therefore the user does not need to worry about this.

```fortran
!$omp parallel do default(shared), private(df), schedule(dynamic), reduction(+:sum)
DO df=1,p_proxy%vspace%get_last_dof_owned()
```

## Reproducible reductions ##

According to the OpenMP specification, an OpenMP reduction does not
need to guarantee the order in which the operations in a reduction
take place. Therefore an OpenMP reduction might return different
results from one run to the next due to rounding errors when summing
floating point values in different orders.

This can cause problems for correctness checking and debugging in
scientific codes. In fact the ability to get reproducible results (via
a switch in the code) for the same number of threads is actually a
requirement in the current Met Office model.

PSyclone therefore provides an option to compute reductions in a
reproducible way. This implementation requires the use of `OMP
PARALLEL` and `OMP LOOP` directives rather than the single `OMP
PARALLEL LOOP` directive that we have been using so far.

Let's modify the script. Create the following two new transformations
next to the existing ones:

```python
    ptrans = OMPParallelTrans()
    ltrans = Dynamo0p3OMPLoopTrans()
```

Replace the following line in the script:

```python
                otrans.apply(loop)
```

with

```python
                if loop.reductions():
                    ptrans.apply(loop)
                    ltrans.apply(loop)
                else:
                    otrans.apply(loop)
```

The `reductions()` method in a loop node is handy here!

If you rerun:

```bash
    $ psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py helmholtz_solver_alg_mod.x90 -d ../code
```

You should see that the psy-layer PSyIR that is output to the screen
now has an `OMP parallel` and an `OMP do` directive around the last
loop. You should also see the `reprod=False` clause.

Take a look at the generated code

```bash
    $ my_fav_editor psy.f90.
```

The generated code is very similar to what we were generating before,
it just has two directives around the reduction loop rather than
one.

Now we can specify that we would like a reproducible reduction by
supplying an option to the transformation.

In the script replace:

```python
                    ltrans.apply(loop)
```

with

```python
                    ltrans.apply(loop, {"reprod": True})
```

The PSyIR that is output to the screen should now specify that
`reprod=True` for the `OMP DO` directive.

Take a look at the generated code:

```bash
    $ my_fav_editor psy.f90.
```

In this version, the values for each thread are summed separately in
the parallel loop and these partial sums are then added together
serially at the end. This is a more complicated solution but PSyclone
takes care of it without the user needing to worry.

If you look at the allocation of the `l_sum array`, you will see that it
is padded by `8`. This is to avoid false sharing between the threads. If
this value needs to be changed it can be modified in the
configuration file. A copy of the configuration file is provided in
this directory if you would like to try this. Modify, the
`REPROD_PAD_SIZE` to whatever value you think appropriate. You can
select to use this configuration file by adding `--config psyclone.cfg`
to the `psyclone` command i.e.

```bash
    $ psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py helmholtz_solver_alg_mod.x90 -d ../code --config psyclone.cfg
```

Notice that you could also set `REPRODUCIBLE_REDUCTIONS` to `true` in the
config file rather than setting it in the script. This would make all
reductions reproducible by default, rather than selectively doing it
in the script. You can try this out if you like by removing the reprod
option in the script, changing the config file and re-running.

## Finished ##

Well done, you have finished the LFRic OpenMP part of the tutorial.